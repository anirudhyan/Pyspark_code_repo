1. Creating a Spark session that is entry point to spark functionality
from pyspark.sql import SparkSession
spark = SparkSession.builder.master('yarn').appName('pyspark_learning').getOrCreate()
# Sparksession comprises of Sparkcontext, SqlContext,HiveContext and Streaming Context in one house
#To access the spark context use spark.SparkContext where spark is spark session object
spark.SparkContext.appName
spark.SparkContext.master
There can be multiple spark sessions in a jvm but can have only 1 sparkcontext

2. Reading of Files from HDFS/S3 for different formats 
#spark.read method is used to create a dataframe reader object which has multiple apis to read multiple source files and format
# Reading a CSV file
inp_df = spark.read.format('csv').option('header',true).
